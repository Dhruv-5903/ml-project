{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjIWMGNigB6M",
        "outputId": "6c639f94-5444-45e6-c45d-6f3763e7c9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            " Artificial Intelligence is changing the world. NLP helps machines understand language. Students learn tokenization, POS tagging, and NER. Preprocessing improves model performance. Practice makes perfect!\n",
            "\n",
            "Sentences:\n",
            "- Artificial Intelligence is changing the world.\n",
            "- NLP helps machines understand language.\n",
            "- Students learn tokenization, POS tagging, and NER.\n",
            "- Preprocessing improves model performance.\n",
            "- Practice makes perfect!\n",
            "\n",
            "Word Tokens: ['Artificial', 'Intelligence', 'is', 'changing', 'the', 'world', 'NLP', 'helps', 'machines', 'understand', 'language', 'Students', 'learn', 'tokenization', 'POS', 'tagging', 'and', 'NER', 'Preprocessing', 'improves', 'model', 'performance', 'Practice', 'makes', 'perfect']\n",
            "Cleaned Tokens: ['artificial', 'intelligence', 'is', 'changing', 'the', 'world', 'nlp', 'helps', 'machines', 'understand', 'language', 'students', 'learn', 'tokenization', 'pos', 'tagging', 'and', 'ner', 'preprocessing', 'improves', 'model', 'performance', 'practice', 'makes', 'perfect']\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# TASK 1: Tokenization & Basic Preprocessing\n",
        "# -----------------------------------------\n",
        "\n",
        "import re\n",
        "\n",
        "text = (\n",
        "    \"Artificial Intelligence is changing the world. \"\n",
        "    \"NLP helps machines understand language. \"\n",
        "    \"Students learn tokenization, POS tagging, and NER. \"\n",
        "    \"Preprocessing improves model performance. \"\n",
        "    \"Practice makes perfect!\"\n",
        ")\n",
        "\n",
        "# Sentence Tokenization (simple split)\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "\n",
        "# Word Tokenization\n",
        "word_tokens = []\n",
        "for sent in sentences:\n",
        "    toks = re.findall(r\"\\b\\w+'?\\w*\\b\", sent)\n",
        "    word_tokens.extend(toks)\n",
        "\n",
        "# Lowercasing & punctuation removal (already clean tokens)\n",
        "cleaned_tokens = [w.lower() for w in word_tokens]\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nSentences:\")\n",
        "for s in sentences:\n",
        "    print(\"-\", s)\n",
        "\n",
        "print(\"\\nWord Tokens:\", word_tokens\n",
        ")\n",
        "print(\"Cleaned Tokens:\", cleaned_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4-bU-KIiDBf",
        "outputId": "f1b26878-02dc-4252-a2c0-43e44609c38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS Tags:\n",
            "('artificial', 'JJ')\n",
            "('intelligence', 'NN')\n",
            "('is', 'VB')\n",
            "('changing', 'VB')\n",
            "('the', 'NN')\n",
            "('world', 'NN')\n",
            "('nlp', 'NN')\n",
            "('helps', 'VB')\n",
            "('machines', 'NN')\n",
            "('understand', 'NN')\n",
            "('language', 'NN')\n",
            "('students', 'NN')\n",
            "('learn', 'VB')\n",
            "('tokenization', 'NN')\n",
            "('pos', 'NN')\n",
            "('tagging', 'VB')\n",
            "('and', 'NN')\n",
            "('ner', 'NN')\n",
            "('preprocessing', 'VB')\n",
            "('improves', 'VB')\n",
            "('model', 'NN')\n",
            "('performance', 'NN')\n",
            "('practice', 'NN')\n",
            "('makes', 'VB')\n",
            "('perfect', 'NN')\n",
            "\n",
            "Counts -> Nouns: 16, Verbs: 8, Adjectives: 1\n",
            "\n",
            "NOTE: NLTK models not found, using fallback POS tagger.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# TASK 2: POS Tagging (NLTK or fallback)\n",
        "# -----------------------------------------\n",
        "\n",
        "import re\n",
        "\n",
        "# Using the cleaned tokens from Task 1\n",
        "tokens = cleaned_tokens\n",
        "\n",
        "pos_tags = []\n",
        "nltk_warning = None\n",
        "used_nltk = False\n",
        "\n",
        "# Try NLTK first\n",
        "try:\n",
        "    import nltk\n",
        "\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    except LookupError:\n",
        "        nltk_warning = \"NLTK models not found, using fallback POS tagger.\"\n",
        "\n",
        "    if nltk_warning is None:\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "        used_nltk = True\n",
        "\n",
        "except Exception as e:\n",
        "    nltk_warning = f\"NLTK not available: {e}\"\n",
        "\n",
        "# Fallback POS Tagging (rule-based)\n",
        "if not used_nltk:\n",
        "    common_verbs = {\n",
        "        \"is\",\"are\",\"was\",\"were\",\"be\",\"helps\",\n",
        "        \"help\",\"learn\",\"learns\",\"changing\",\"makes\",\"improves\"\n",
        "    }\n",
        "\n",
        "    pos_tags = []\n",
        "    for w in tokens:\n",
        "        if w in common_verbs or w.endswith(\"ing\") or w.endswith(\"ed\"):\n",
        "            pos_tags.append((w, \"VB\"))\n",
        "        elif w.endswith(\"ly\"):\n",
        "            pos_tags.append((w, \"RB\"))\n",
        "        elif w.endswith(\"al\") or w.endswith(\"ive\") or w.endswith(\"ous\"):\n",
        "            pos_tags.append((w, \"JJ\"))\n",
        "        else:\n",
        "            pos_tags.append((w, \"NN\"))\n",
        "\n",
        "# Counting\n",
        "nouns = sum(1 for _,t in pos_tags if t.startswith(\"NN\"))\n",
        "verbs = sum(1 for _,t in pos_tags if t.startswith(\"VB\"))\n",
        "adjectives = sum(1 for _,t in pos_tags if t.startswith(\"JJ\"))\n",
        "\n",
        "print(\"POS Tags:\")\n",
        "for p in pos_tags:\n",
        "    print(p)\n",
        "\n",
        "print(f\"\\nCounts -> Nouns: {nouns}, Verbs: {verbs}, Adjectives: {adjectives}\")\n",
        "\n",
        "if nltk_warning:\n",
        "    print(\"\\nNOTE:\", nltk_warning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8jRjLiYiIiW",
        "outputId": "c2325b89-907d-4c54-d6d6-601477520337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Named Entities Found:\n",
            "('Artificial Intelligence', 'ORG')\n",
            "('Students', 'PERSON')\n",
            "\n",
            "NOTE: spaCy model not available: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# TASK 3: Named Entity Recognition (NER)\n",
        "# -----------------------------------------\n",
        "\n",
        "import re\n",
        "\n",
        "text_for_ner = (\n",
        "    \"Artificial Intelligence is changing the world. \"\n",
        "    \"NLP helps machines understand language. \"\n",
        "    \"Students learn tokenization, POS tagging, and NER.\"\n",
        ")\n",
        "\n",
        "ner_results = []\n",
        "used_spacy = False\n",
        "spacy_warning = None\n",
        "\n",
        "# Try spaCy first\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        doc = nlp(text_for_ner)\n",
        "        for ent in doc.ents:\n",
        "            ner_results.append((ent.text, ent.label_))\n",
        "        used_spacy = True\n",
        "    except Exception as e:\n",
        "        spacy_warning = f\"spaCy model not available: {e}\"\n",
        "\n",
        "except Exception as e:\n",
        "    spacy_warning = f\"spaCy not installed: {e}\"\n",
        "\n",
        "# Fallback NER if spaCy unavailable\n",
        "if not used_spacy:\n",
        "    fallback_entities = []\n",
        "\n",
        "    # Simple rule: extract Capitalized words or phrases\n",
        "    caps = re.findall(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b', text_for_ner)\n",
        "\n",
        "    org_list = {\"NLP\", \"Artificial Intelligence\"}\n",
        "\n",
        "    for c in caps:\n",
        "        if c in org_list:\n",
        "            fallback_entities.append((c, \"ORG\"))\n",
        "        elif c == \"Students\":\n",
        "            fallback_entities.append((c, \"PERSON\"))\n",
        "        else:\n",
        "            fallback_entities.append((c, \"MISC\"))\n",
        "\n",
        "    # Remove duplicates\n",
        "    ner_results = list(dict.fromkeys(fallback_entities))\n",
        "\n",
        "print(\"Named Entities Found:\")\n",
        "for ent in ner_results:\n",
        "    print(ent)\n",
        "\n",
        "if spacy_warning:\n",
        "    print(\"\\nNOTE:\", spacy_warning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AahoDeAOiMny",
        "outputId": "2b967274-a72d-4cbc-d363-ba0d2cf6bda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Tokens:\n",
            "['l o w e r </w>', 'n e w e r </w>', 's l o w e r </w>']\n",
            "\n",
            "BPE Merge History:\n",
            "\n",
            "Iteration 1:\n",
            "Merged Pair: ('w', 'e')\n",
            "Frequency: 3\n",
            "Tokens After Merge: ['l o we r </w>', 'n e we r </w>', 's l o we r </w>']\n",
            "\n",
            "Iteration 2:\n",
            "Merged Pair: ('we', 'r')\n",
            "Frequency: 3\n",
            "Tokens After Merge: ['l o wer </w>', 'n e wer </w>', 's l o wer </w>']\n",
            "\n",
            "Iteration 3:\n",
            "Merged Pair: ('wer', '</w>')\n",
            "Frequency: 3\n",
            "Tokens After Merge: ['l o wer</w>', 'n e wer</w>', 's l o wer</w>']\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# TASK 4: Byte Pair Encoding (BPE)\n",
        "# -----------------------------------------\n",
        "\n",
        "def bpe_merge_steps(sentence, iterations=3):\n",
        "    # Split into words, then split words into chars + </w>\n",
        "    words = sentence.split()\n",
        "    tokens = [\" \".join(list(w)) + \" </w>\" for w in words]\n",
        "    tokens = [t.split() for t in tokens]  # list of lists\n",
        "\n",
        "    merge_history = []\n",
        "\n",
        "    for it in range(iterations):\n",
        "        # Count all adjacent pairs\n",
        "        pair_freq = {}\n",
        "        for token in tokens:\n",
        "            for i in range(len(token) - 1):\n",
        "                pair = (token[i], token[i+1])\n",
        "                pair_freq[pair] = pair_freq.get(pair, 0) + 1\n",
        "\n",
        "        if not pair_freq:\n",
        "            break\n",
        "\n",
        "        # Find most frequent pair\n",
        "        best_pair = max(pair_freq, key=pair_freq.get)\n",
        "        freq = pair_freq[best_pair]\n",
        "\n",
        "        # Merge step\n",
        "        new_tokens = []\n",
        "        for token in tokens:\n",
        "            new_token = []\n",
        "            i = 0\n",
        "            while i < len(token):\n",
        "                # If a pair matches, merge them\n",
        "                if i < len(token) - 1 and token[i] == best_pair[0] and token[i+1] == best_pair[1]:\n",
        "                    new_token.append(token[i] + token[i+1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_token.append(token[i])\n",
        "                    i += 1\n",
        "            new_tokens.append(new_token)\n",
        "\n",
        "        merge_history.append({\n",
        "            \"iteration\": it + 1,\n",
        "            \"merged_pair\": best_pair,\n",
        "            \"frequency\": freq,\n",
        "            \"tokens_after_merge\": [\" \".join(t) for t in new_tokens]\n",
        "        })\n",
        "\n",
        "        tokens = new_tokens\n",
        "\n",
        "    # Initial token list\n",
        "    initial = [\" \".join(list(w)) + \" </w>\" for w in sentence.split()]\n",
        "\n",
        "    return {\"initial\": initial, \"history\": merge_history}\n",
        "\n",
        "\n",
        "# ----- Run BPE on sample text -----\n",
        "sentence = \"lower newer slower\"\n",
        "bpe_result = bpe_merge_steps(sentence, iterations=3)\n",
        "\n",
        "print(\"Initial Tokens:\")\n",
        "print(bpe_result[\"initial\"])\n",
        "\n",
        "print(\"\\nBPE Merge History:\")\n",
        "for h in bpe_result[\"history\"]:\n",
        "    print(f\"\\nIteration {h['iteration']}:\")\n",
        "    print(\"Merged Pair:\", h[\"merged_pair\"])\n",
        "    print(\"Frequency:\", h[\"frequency\"])\n",
        "    print(\"Tokens After Merge:\", h[\"tokens_after_merge\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ooAM6aaiRbE",
        "outputId": "8c469aaf-7942-4a72-887a-41bce246868c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word 1: sunday\n",
            "Word 2: saturday\n",
            "\n",
            "Minimum Edit Distance: 3\n",
            "\n",
            "Operations:\n",
            "('match', 's')\n",
            "('insert', 'a')\n",
            "('insert', 't')\n",
            "('match', 'u')\n",
            "('substitute', ('n', 'r'))\n",
            "('match', 'd')\n",
            "('match', 'a')\n",
            "('match', 'y')\n",
            "\n",
            "DP Matrix:\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "[1, 0, 1, 2, 3, 4, 5, 6, 7]\n",
            "[2, 1, 1, 2, 2, 3, 4, 5, 6]\n",
            "[3, 2, 2, 2, 3, 3, 4, 5, 6]\n",
            "[4, 3, 3, 3, 3, 4, 3, 4, 5]\n",
            "[5, 4, 3, 4, 4, 4, 4, 3, 4]\n",
            "[6, 5, 4, 4, 5, 5, 5, 4, 3]\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# TASK 5: Minimum Edit Distance (MED)\n",
        "# -----------------------------------------\n",
        "\n",
        "def min_edit_distance(a, b):\n",
        "    la = len(a)\n",
        "    lb = len(b)\n",
        "\n",
        "    # Create DP matrix\n",
        "    dp = [[0] * (lb + 1) for _ in range(la + 1)]\n",
        "\n",
        "    # Base cases\n",
        "    for i in range(la + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(lb + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    # Fill matrix\n",
        "    for i in range(1, la + 1):\n",
        "        for j in range(1, lb + 1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j] + 1,      # deletion\n",
        "                dp[i][j-1] + 1,      # insertion\n",
        "                dp[i-1][j-1] + cost  # substitution\n",
        "            )\n",
        "\n",
        "    # Backtrace operations\n",
        "    i, j = la, lb\n",
        "    operations = []\n",
        "\n",
        "    while i > 0 or j > 0:\n",
        "        if i > 0 and dp[i][j] == dp[i-1][j] + 1:\n",
        "            operations.append((\"delete\", a[i-1]))\n",
        "            i -= 1\n",
        "        elif j > 0 and dp[i][j] == dp[i][j-1] + 1:\n",
        "            operations.append((\"insert\", b[j-1]))\n",
        "            j -= 1\n",
        "        else:\n",
        "            if a[i-1] == b[j-1]:\n",
        "                operations.append((\"match\", a[i-1]))\n",
        "            else:\n",
        "                operations.append((\"substitute\", (a[i-1], b[j-1])))\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "\n",
        "    operations.reverse()\n",
        "    return dp, dp[la][lb], operations\n",
        "\n",
        "\n",
        "# ---- Example Run ----\n",
        "word1 = \"sunday\"\n",
        "word2 = \"saturday\"\n",
        "\n",
        "dp_matrix, distance, ops = min_edit_distance(word1, word2)\n",
        "\n",
        "print(\"Word 1:\", word1)\n",
        "print(\"Word 2:\", word2)\n",
        "print(\"\\nMinimum Edit Distance:\", distance)\n",
        "\n",
        "print(\"\\nOperations:\")\n",
        "for op in ops:\n",
        "    print(op)\n",
        "\n",
        "print(\"\\nDP Matrix:\")\n",
        "for row in dp_matrix:\n",
        "    print(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ7P_yaxiWXY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
